{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e81c1162-7ad7-4a90-a1aa-3825e6a7bb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a7fa64-3ab5-43b4-902e-f6c2ea2a1c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc2d45be-0eb6-45cb-a572-52b95934e528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "Create catalog If not exists telecom_catalog_assign;\n",
    "Create Schema If not exists telecom_catalog_assign.landing_zone;\n",
    "Create volume If not exists telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123f5053-9adc-4376-b0bf-b3cc2a36fe13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cb39b32-cbc5-4b1d-98e6-318fc15e0a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Telecom Domain Read & Write Ops Assignment - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes.\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418b22e9-0053-4820-b3f6-3f85ebe20bda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f2d16d-6952-40d7-a0cf-1657bfc1691b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write dbutils.fs code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d35ca02d-8558-434d-ae90-31ddcf629499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", customer_csv, True)\n",
    "\n",
    "if dbutils.fs.ls(\"dbfs:/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\"):\n",
    "    print(\"Success: File copied.\")\n",
    "else:\n",
    "    print(\"Error: File not found.\")\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count 101\\t320\\t1500\\t20 102\\t120\\t4000\\t5 103\\t540\\t600\\t52 104\\t45\\t200\\t2 105\\t0\\t0\\t0 '''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\", usage_tsv, True)    \n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\", tower_logs_region1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a77da1-0ebc-4d3f-a1a1-5a9b24f402e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "547b23a7-8dc8-4fd0-b4ab-07f0dc011616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Spark Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2994f62d-5bd6-4a9d-9235-d894edb62686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading only CSV files from a directory\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .option(\"pathGlobFilter\", \"*.csv\")  # Only matches files ending in .csv\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39eff24c-beb1-4e76-afba-780e579efd4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read.options(header=False, inferSchema=True).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\")\n",
    "      ).toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "display(df)\n",
    "display(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc27a0f-30d8-4de5-ad5a-6d3650580008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=(spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")).toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "\n",
    "df2=(spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_1.csv\")).toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "combined_df = df1.unionByName(df2)\n",
    "# display(combined_df)\n",
    "\n",
    "(df2.write\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \"|\")\n",
    "  .mode(\"overwrite\")\n",
    "  .save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_2.csv\"))\n",
    " \n",
    "df3=spark.read.option(\"sep\", \"|\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_2.csv\")\n",
    "df3.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e362511-3473-4f44-826b-0dfc4c803069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.help()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2580a829-f20a-49b1-b246-5441acd05c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###Option 1. Using simple string format of define schema. (least used because it will not support some complex formats)\n",
    "schema_Stru=\"customer_id INT, name STRING, age INT, city STRING, plan STRING\"\n",
    "df = (spark.read\n",
    "      .format(\"csv\")\n",
    "      .schema(schema_Stru)\n",
    "      .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "      )\n",
    "###Option 2. Using StructType to define schema. (Recommended) /Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\n",
    "display(df)\n",
    "df.show(3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b014ec-a67c-4bef-ac8f-c5c2d1685714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTANT: 2. Using structure type to define schema.\n",
    "#For the given data \"id,fname,lname,age,prof\" i am going to use StructType(StructField(\"id\",IntegerType()),StructField(\"fname\",StringType())...)\n",
    "#Pattern: StructType([StructField(\"colname\",DataType()),StructField(\"colname\",DataType())......])\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "str1_schema = StructType([\n",
    "     StructField(\"customer_id\",  IntegerType(), True),\n",
    "     StructField(\"name\",  StringType(), True),\n",
    "     StructField(\"age\",  IntegerType(), True),\n",
    "     StructField(\"city\",  StringType(), True)])\n",
    "dfstru=spark.read.schema(str1_schema).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "dfstru.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6ef9e1b-f874-47f4-8629-2df2e7a9f6f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Reading data from json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3ff92b5a-b494-4329-bf2e-3f8dbf8d6829",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766389075339}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json=spark.read.option(\"allowComments\", \"True\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/JSON FIle/SampleJson.txt\",)\n",
    "display(df_json.take(10))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eff1c66-82c0-4584-9e5d-01142d26c88e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json=spark.read.option(\"allowComments\", \"True\").option(\"escape\", \"~\").json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/JSON FIle/SampleJson_2.txt\",)\n",
    "display(df_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5932463667430858,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Telecom Domain Read & Write Ops Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
