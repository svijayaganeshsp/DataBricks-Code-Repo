{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b9569db-fbd0-4fa4-9f8c-b4fe9b279cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###We are going to learn some advanced options available in the different file/table read functions (Least bother)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a070bc4-17e0-4059-97ef-0e29d09c8cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Sample data\n",
    "#this is a retail data\n",
    "productId,productName,stdCost,stdPrice,effDt,dt\n",
    "PURA100,\"Pure Soft Detergent - ~\"100ml~\"\", 1.50 , 3.00,2019-12-03 00:00:01,2019-12-01\n",
    "PURA200,\"Pure Soft \n",
    "Detergent \n",
    "- 200ml\", 2.00 , 3.99,2019-03-01 00:00:01,2019-01-01\n",
    "PURA250,\"Pure Soft Detergent - 250ml\", 2.30 , 4.50,2019-03-01 00:00:01,2019-01-01\n",
    "PURA500,\"Pure Soft Detergent - 500ml\", 3.50 , 6.50,2019-03-02 00:00:01,2019-01-01\n",
    "DETA100,\"Detafast Stain Remover - 100ml\", 3.00 , 6.00,2019-03-01 00:00:01,2019-01-01\n",
    "DETA200,\"Detafast Stain Remover - 200ml\", 3.50 , 6.50,2019-03-03 00:00:01,2019-01-01\n",
    "DETA800,na, 6.00 ,-1,2019-03-04 00:00:01,2019-01-01\n",
    "SUPA101,\"Super Soft - Product Sample\", 0.30 ,0,2019-03-06 00:00:01,2019-01-01\n",
    "SUPA102,\"Super Soft - 250ml \", 2.50 , 4.50,2019-03-01 00:00:01,2019-01-01\n",
    "SUPA103,, 3.50 , 6.99 ,2019-04-01 00:00:01,2019-01-01\n",
    ",\"Super Soft - 1 Litre\", 5.00 , 9.99,2019-04-02 00:00:01,2019-04-20,2019-01-01\n",
    " SUPA105 , \"Super Soft Bulk - 2 Litres\"   , 8.00 , 14.50,2019-04-03 00:00:01,2019-01-01\n",
    "Deta10,\"Pure Soft Detergent - ~\"100ml~\"\", 1.50 ,3.00,2019-12-03 00:00:01,2019-12-01\n",
    "#end of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c96d50-7254-47d0-9b35-76c8c44bb49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Important - There are 3 typical read modes and the default read mode is permissive.\n",
    "##### 1. permissive — All fields are set to null and corrupted records are placed in a string column called _corrupt_record\n",
    "##### \t2. dropMalformed — Drops all rows containing corrupt records.\n",
    "##### 3. failFast — Fails when corrupt records are encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abfd4985-c4c5-4f3e-b780-8683dc3acd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####CSV Advanced Feature - Very Important, Important, Not Important (just try to know once for [all](url))\n",
    "**Very Important:** path, schema, sep, header, inferSchema, samplingRatio, mode, columnNameOfCorruptRecord <br>\n",
    "**Important:** quote, escape,comment, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, dateFormat, timestampFormat, multiLine, enforceSchema,pathGlobFilter, recursiveFileLookup,modifiedBefore,modifiedAfter<br>\n",
    "**Not Important:** encoding, positiveInf, negativeInf, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, charToEscapeQuoteEscaping, emptyValue, locale, lineSep, unescapedQuoteHandling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1854fab2-552e-4107-ae3c-f771bc07f081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"cid string,name string,amt string,dop string\"\n",
    "df1=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/malformeddata.txt\",header=False,sep=\",\"\n",
    "                                   ,comment='#')#permit malformed records and allow me to do a RCA (Root cause analysis)\n",
    "df1.where(\"upper(_c2)=lower(_c2)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56276266-7136-43df-ac73-2ba13d1c5c8f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"corrupt_record\":332},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766210633310}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#**Very Important:** path, schema, sep, header, inferSchema, samplingRatio, mode, columnNameOfCorruptRecord <br>\n",
    "struct1=\"cid int,name string,amt decimal(10,2),dop date\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewe47_datalake/malformeddata.txt\",header=False,sep=\",\"\n",
    "                                   ,comment='#',mode='dropMalformed')#Blindly drop malformed records\n",
    "display(df1)                                   \n",
    "struct1=\"cid int,name string,amt decimal(10,2),dop date,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewe47_datalake/malformeddata.txt\",header=False,sep=\",\"\n",
    "                                   ,comment='#',mode='permissive',columnNameOfCorruptRecord='corrupt_record')#permit malformed records and allow me to do a RCA (Root cause analysis)\n",
    "display(df1.where(\"corrupt_record is not null\"))\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewe47_datalake/malformeddata.txt\",header=False,sep=\",\"\n",
    "                                   ,comment='#',mode='failFast')#Fail immediately when malformed records occurs\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2550e913-906e-4d0d-830c-b8d6ffa312a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#**Important:** quote, escape,comment, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace,  dateFormat, timestampFormat, multiLine, enforceSchema,pathGlobFilter, recursiveFileLookup,modifiedBefore,modifiedAfter, nullValue, nanValue<br>\n",
    "struct1=\"cid int,name string,amt decimal(10,2),dop date\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewe47_datalake/malformeddata1.txt\",header=False,sep=\",\"\n",
    ",comment='#',quote=\"'\",escape='|',ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,dateFormat='yyyy-dd-MM',mode=\"permissive\",modifiedAfter='2025-12-20',multiLine=True,nullValue='na')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28a2681a-c4a6-4680-8f81-17b58b8a4ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####JSON Advanced Feature - \n",
    "**Very Important** - path,schema,columnNameOfCorruptRecord,dateFormat,timestampFormat,multiLine,pathGlobFilter,recursiveFileLookup<br>\n",
    "No header, No inferSchema, No sep in json...<br>\n",
    "**Important** - primitivesAsString(consider all cols as string), prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, lineSep, samplingRatio, dropFieldIfAllNull, modifiedBefore, modifiedAfter, useUnsafeRow(This is performance optimization when the data is loaded into spark memory) <br>\n",
    "**Not Important** (just try to know once for all) - allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, allowUnquotedControlChars, encoding, locale, allowNonNumericNumbers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0d72e74-5e9b-4ae9-be23-73ba0cf3cb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json=spark.read.json(\"/Volumes/workspace/default/volumewe47_datalake/we47_source/simple_json.txt\",samplingRatio=0.1,prefersDecimal=True)\n",
    "#primitivesAsString\n",
    "df_json.printSchema()\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fd51c7-1fc5-490f-b157-48d367d9b433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id int,name string,amt float, dop date,custom_corruptrow string\"\n",
    "df_json=spark.read.json(\"/Volumes/workspace/default/volumewe47_datalake/we47_source/simple_json2.txt\",allowComments=True,lineSep='~',allowSingleQuotes=True,allowUnquotedFieldNames=True,columnNameOfCorruptRecord=\"custom_corruptrow\",schema=strt1,dateFormat='yyyy-dd-MM')\n",
    "#primitivesAsString\n",
    "df_json.printSchema()\n",
    "df_json.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919c533c-7ac6-43e7-b95e-7be250ce0749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id int,name string,amt float, dop date,custom_corruptrow string\"\n",
    "df_json=spark.read.json(\"/Volumes/workspace/default/volumewe47_datalake/we47_source/simple_json_multiline3.txt\",allowComments=True,allowSingleQuotes=True,allowUnquotedFieldNames=True,columnNameOfCorruptRecord=\"custom_corruptrow\",schema=strt1,dateFormat='yyyy-dd-MM',multiLine=True)\n",
    "#primitivesAsString\n",
    "df_json.printSchema()\n",
    "df_json.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09e6001-31a8-47dd-9f47-ddb9d1b0eac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Serialized data Advanced Feature - orc, parquet/delta (very very important & we learn indepth)\n",
    "- PathOrPaths\n",
    "- **mergeSchema** - Important interview property (make it proactive/make it driven in the interview) SCHEMA EVOLUTION\n",
    "- pathGlobFilter\n",
    "- recursiveFileLookup\n",
    "- modifiedBefore\n",
    "- modifiedAfter\n",
    "Problem statement:\n",
    "Source is sending data in any way they want...\n",
    "Day1/source1- 5 cols\n",
    "Day2/source2 - 7 Cols\n",
    "\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b729303d-7c6f-42b8-b4eb-b808039c2005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/sampledata/\",inferSchema=True,header=True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec309598-a4f8-4573-9038-b4c80d96f9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "problem:\n",
    "Source is sending data in any way they want...\n",
    "Day1/source1- 5 cols\n",
    "Day2/source2 - 7 Cols\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507dd300-9c46-4e1c-b1d8-3324a8844eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I am converting the CSV data into ORC data format to achieve SCHEMA EVOLUTION(CHANGING)\n",
    "spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/sampledata/source1.txt\",inferSchema=True,header=True).write.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata_merged/\")\n",
    "spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/sampledata/source2.txt\",inferSchema=True,header=True).write.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata_merged/\",mode='append')\n",
    "spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/source3.txt\",inferSchema=True,header=True).write.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata_merged/\",mode='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7b5fa7-c907-417d-8d75-e1f877c41e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orc_merged_data=spark.read.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata_merged/\",mergeSchema=True)\n",
    "display(orc_merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8a24e2-2636-4cc9-a8a1-3179089c59e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I am converting the CSV data into ORC data format to achieve SCHEMA EVOLUTION(CHANGING)\n",
    "spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/sampledata/source1.txt\",inferSchema=True,header=True).write.parquet(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/parquet_targetdata_merged/\")\n",
    "spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/sampledata/source2.txt\",inferSchema=True,header=True).write.parquet(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/parquet_targetdata_merged/\",mode='append')\n",
    "spark.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/source3.txt\",inferSchema=True,header=True).write.parquet(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/parquet_targetdata_merged/\",mode='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7afb26-8a03-4317-b9ac-bdc49b7bec9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_parquet=spark.read.parquet(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/parquet_targetdata_merged/\",mergeSchema=True)\n",
    "display(df_parquet)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Advanced-Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
