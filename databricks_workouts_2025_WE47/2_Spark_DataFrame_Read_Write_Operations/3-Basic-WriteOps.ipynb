{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3756d01-4aa7-45d1-bffa-b7e3afd67e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#By Knowing this notebook, we can become a eligible \"DATA EGRESS DEVELOPER\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71713fcb-b659-4e62-bbee-1d3092f13683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Let's get some data we have already..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059b8ec6-31ff-46f7-a19d-069340242a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.read.csv(path=\"/Volumes/we47catalog/we47schema/we47_volume/we47_dir1/custs_header\",header=True,inferSchema=True)\n",
    "df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4de366d1-b620-4eb3-8014-943802d1b67a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing the data in Builtin - different file formats & different targets (all targets in this world we can write the data also...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "265d5989-ca7e-4f2f-8e6a-93e75c23948d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Writing in csv format with few basic options listed below\n",
    "- header\n",
    "- sep\n",
    "- mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ae6f6b-aae1-4749-ad83-da37c28e41bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#We did a schema migration from comma to tilde delimiter\n",
    "df1.write.csv(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/csv_targetdata\",header=True,sep='~',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a29a4365-30ff-49c1-af16-5ddbbaa9b3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Writing in json format with few basic options listed below (Schema migration/Data Conversion)\n",
    "path<br>\n",
    "mode\n",
    "- We did a schema migration and data conversion from csv to json format (ie structued to semi structured format)\n",
    "- json - we learn a lot subsequently, \n",
    "- what is json - fundamentally it is a dictionary of dictionaries\n",
    "- json - java script object notation\n",
    "- format - {\"k1\":v1,\"k2\":v2,\"k3\":v2} where key has to be unique & enclosed in double quotes and value can be anything\n",
    "- **when to go with json or benifits** - \n",
    "- a. If we have data in a semistructure format(variable data format with dynamic schema)\n",
    "- b. columns and the types and the order can be different\n",
    "- c. json will be provided by the sources if the data is dynamic in nature or if the data is api response in nature.\n",
    "- d. json is a efficient data format (serialized/encoded) for performing data exchange between applications via network & good for parsing also.\n",
    "- e. json can be used to group or create hierarchy of data in a complex or in a nested format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f04f4317-79d6-4fe5-98e8-f41727c31739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.json(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/json_targetdata\",mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9b6ca7-9aa4-4d18-bb18-0d62829dddd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3.Serialization & Deserialization File formats (Brainy File formats)\n",
    "What are the (builtin) serialized file formats we are going to learn?\n",
    "orc\n",
    "parquet\n",
    "delta(databricks properatory)\n",
    "\n",
    "- We did a schema migration and data conversion from csv/json to serialized data format (ie structued to sturctured(internall binary unstructured) format)\n",
    "- We learn/use a lot/heavily subsequently, \n",
    "- what is serialized - fundamentally they are intelligent/encoded/serialized/binary data formats applied with lot of optimization & space reduction strategies..\n",
    "- orc - optimized row column format\n",
    "- parquet - tiled data format\n",
    "- delta(databricks properatory) enriched parquet format - Delta (modified) operations can be performed\n",
    "- format - serialized/encoded , we can't see with mere eyes, only some library is used deserialized/decoded data can be accessed as structured data\n",
    "- **when to go with serialized or benifits** - \n",
    "- a. For storage benifits for eg. orc will save 65+% of space for eg. if i store 1gb data it occupy 350 space, with compression it can improved more...\n",
    "- b. For processing optimization. Orc/parquet/delta will provide the required data alone if you query using Pushdown optimization .\n",
    "- c. Interoperability feature - this data format can be understandable in multiple environments for eg. bigquery can parse this data.\n",
    "- d. Secured\n",
    "- **In the projects/environments when to use what fileformats - we learn in detail later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fb6848-a995-4977-a1d0-ff547c686cd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata\",mode='ignore')#serialization\n",
    "spark.read.orc(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/orc_targetdata\").show(2)#deserialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed8a769-b528-44ee-879f-b8c145e72c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.option(\"maxRecordsPerFile\",1).parquet(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/parquet_targetdata2\",mode='error',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ff59ea-6793-4d25-9896-a14460d241a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df1.write.delta(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/delta_targetdata\")\n",
    "df1.write.format(\"delta\").save(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/delta_targetdata\",mode='overwrite')\n",
    "spark.read.format(\"delta\").load(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/delta_targetdata\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1d83f7-b903-42f8-aa39-3094dba9b94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#What is the default format of file will be generated with, when we don't mention the format explicitly?\n",
    "#It is Parquet(Delta)\n",
    "df1.write.save(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/what_targetdata\",mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bb4ab26-481f-4b00-a5cb-675b105863d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4.Table Load Operations - Building LAKEHOUSE ON TOP OF DATALAKE\n",
    "Can we do SQL operations directly on the tables like a database or datawarehouse? or Can we build a Lakehouse in Databricks?\n",
    "- We learn/use a lot/heavily subsequently, \n",
    "- what is Lakehouse - A SQL/Datawarehouse/Query layer on top of the Datalake is called Lakehouse\n",
    "- We have different lakehouses which we are going to learn further - \n",
    "1. delta tables (lakehouse) in databricks\n",
    "2. hive in onprem\n",
    "3. bigquery in GCP\n",
    "4. synapse in azure\n",
    "5. athena in aws\n",
    "- **when to go with lakehouse** - \n",
    "- a. Transformation\n",
    "- b. Analysis/Analytics\n",
    "- c. AI/BI\n",
    "- d. Literally we are going to learn SQL & Advanced SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad625815-1e9d-4917-b87a-8e8d756bee72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Out of 18 write.functions, we know 9 functions, lets go with few more basic functions (xml, saveAsTable,InsertInto)\n",
    "df1.write.saveAsTable(\"default.customertbl\",mode='overwrite')#default delta format\n",
    "spark.read.table(\"default.customertbl\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f323284-afde-43f8-9a7c-c0838afa3391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Notes Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores the column names and just uses position-based resolution.\n",
    "# table has to be present already\n",
    "# this will be used for some minimal data write operation hence preferred function is saveAsTable()\n",
    "df1.write.insertInto(\"customertbl\",overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7586d9b9-5766-44e9-9c4a-51c1805f316c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5. XML Format - Semi structured data format (most of the json features can be applied in xml also, but in DE world not so famous like json)\n",
    "- Used rarely on demand (by certain target/source systems eg. mainframes)\n",
    "- Can be related with json, but not so much efficient like json\n",
    "- Databricks provides xml as a inbuild function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e3bc79-afad-4cbb-a84a-1f98f0f06c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1.write.xml(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/xml_targetdata\",rowTag='customer',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c88733-687d-4f52-b864-1eec9a7eb87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.xml(\"/Volumes/workspace/default/volumewe47_datalake/serialized_compressed_data_sources/xml_targetdata\",rowTag='customer').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7abb6500-0b7e-4339-b814-3e356c78d7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Modes in Writing\n",
    "1. **Append** - Adds the new data to the existing data. It does not overwrite anything.\n",
    "2. **Overwrite** - Replaces the existing data entirely at the destination.\n",
    "3. **ErrorIfexist**(default) - Throws an error if data already exists at the destination.\n",
    "4. **Ignore** - Skips the write operation if data already exists at the destination."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7347217471020383,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3-Basic-WriteOps",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
