{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee99bb57-cc17-4aaa-836a-89875ba34790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Try to use this sales data with atleast very important and important options...\n",
    "https://drive.google.com/file/d/1MZI4XIofL-0QpMIr9sFODSKVkexrSZ1A/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4280bcb1-491e-43e6-a978-4c026e7624fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. CSV Advanced Features - \n",
    "######Very Important - path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None,header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, \n",
    "######Important - mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None,  quote: Optional[str]=None, escape: Optional[str]=None, \n",
    "Not Important but good to know once - encoding: Optional[str]=None, comment: Optional[str]=None,ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None,   multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a070bc4-17e0-4059-97ef-0e29d09c8cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### A. Options for handling quotes & Escape\n",
    "\n",
    "id,name,remarks\n",
    "1,'Ramesh, K.P','Good performer'\n",
    "2,'Manoj','Needs ~'special~' attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aeddbb0-2538-47dd-91cf-2b99f09f172b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When to go for quote: If the data is having delimiter in it..\n",
    "#When to go for escape: If the data is having quote in it...\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata1.txt\",header=False,sep=',',mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\",escape=\"|\")\n",
    "df1.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2878e608-057e-4b0d-a33d-3ad862025f23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B. Comments, Multi line, leading and trailing whitespace handling, null and nan handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6006eaa9-ef12-4ed3-a3a7-c665bcafbf5f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766113838409}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "struct1=\"custid int,name string,height float,joindt date,age string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata2.txt\",header=False,mode='permissive'\n",
    "                                   ,multiLine=True,quote=\"'\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,\n",
    "                                   nullValue='na',nanValue=-1,maxCharsPerColumn='100',modifiedAfter='2025-12-19',dateFormat=\"yyyy-dd-MM\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a395acd-7233-49e8-806d-800d56de7195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C. Read modes in csv (Important feature)\n",
    "If any data challenges (malformed data) such as format issue/column numbers (lesser/more than expected) issue etc.,\n",
    "### There are 3 typical read modes and the default read mode is permissive.\n",
    "##### 1. permissive — All fields are set to null and corrupted records are placed in a string column called _corrupt_record\n",
    "##### \t2. dropMalformed — Drops all rows containing corrupt records.\n",
    "##### 3. failFast — Fails when corrupt records are encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7624274-7e89-4410-8be3-1e573ce99bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We learned about few important features mode, columnNameOfCorruptRecord, Quote, Comment\n",
    "#Question - Corrupt_record column consume more memory because it capturing all the column values(incorrect) in one column. ? Useful for doing RCA (Root Cause Analysis/Debugging)\n",
    "struct1=\"custid int,name string,age int,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata.txt\",header=False,sep=',',mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata.txt\",header=False,sep=',',mode='dropMalformed',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/malformeddata.txt\",header=False,sep=',',mode='failFast',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\")\n",
    "df1.show(10)\n",
    "#df1.filter(\"corrupt_record is not null\").write.csv(\"/Volumes/workspace/default/volumewd36/rejecteddata\")\n",
    "#spark.read.csv(\"/Volumes/workspace/default/volumewd36/rejecteddata\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2e99d11-77a0-447f-a67c-4642d48e3c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. JSON Advanced Features - \n",
    "**Very Important** - path,schema,columnNameOfCorruptRecord,dateFormat,timestampFormat,multiLine,pathGlobFilter,recursiveFileLookup<br>\n",
    "No header, No inferSchema, No sep in json...<br>\n",
    "**Important** - primitivesAsString(don't do inferSchema), prefersDecimal, allowComments, allowUnquotedFieldNames, `allowSingleQuotes`, lineSep, samplingRatio, dropFieldIfAllNull, modifiedBefore, modifiedAfter, useUnsafeRow(This is performance optimization when the data is loaded into spark memory) <br>\n",
    "**Not Important** (just try to know once for all) - allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, allowUnquotedControlChars, encoding, locale, allowNonNumericNumbers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025c46c6-fce9-42f3-af44-561bdc00da47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option\n",
    "#primitivesAsString = inferSchema=False\n",
    "dfjson1=spark.read.json(\"/Volumes/workspace/default/volumewd36/\",primitivesAsString=True)\n",
    "dfjson1.printSchema()\n",
    "#prefersDecimal\n",
    "dfjson1=spark.read.json(\"/Volumes/workspace/default/volumewd36/\",prefersDecimal=True)\n",
    "dfjson1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96035ce2-3f08-4037-a7c2-620022c804d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If we don't define structure, it will infer the schema\n",
    "str1=\"id int,name string,amt float,dop date,corruptrecord string\"\n",
    "dfjson1=spark.read.schema(str1).json(\"/Volumes/workspace/default/volumewd36/\",samplingRatio=1,allowUnquotedFieldNames=True,allowSingleQuotes=True,modifiedAfter='2025-12-22',dateFormat='yyyy-dd-MM',columnNameOfCorruptRecord=\"corruptrecord\",pathGlobFilter=\"simple_json.tx*\",recursiveFileLookup=True)\n",
    "dfjson1.printSchema()\n",
    "dfjson1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5963e4-0fa2-466a-a1e7-cb7e5c1ad012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str1=\"id int,name string,amt float,dop date,corruptrecord string\"\n",
    "dfjson1=spark.read.schema(str1).json(\"/Volumes/workspace/default/volumewd36/\",allowUnquotedFieldNames=True,allowSingleQuotes=True,modifiedAfter='2025-12-22',dateFormat='yyyy-dd-MM',columnNameOfCorruptRecord=\"corruptrecord\",pathGlobFilter=\"simple_json2.tx*\",recursiveFileLookup=True,\n",
    "                                     allowComments=True,lineSep='~',useUnsafeRow=True)\n",
    "dfjson1.printSchema()\n",
    "dfjson1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619066f6-9278-42d0-a9b6-497246b6a105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "str1=\"id int,name string,amt float,dop date,corruptrecord string\"\n",
    "dfjson1=spark.read.schema(str1).json(\"/Volumes/workspace/default/volumewd36/simple_json_multiline3.txt\",dateFormat='yyyy-dd-MM',columnNameOfCorruptRecord=\"corruptrecord\",\n",
    "                                     allowComments=True,multiLine=True)\n",
    "dfjson1.printSchema()\n",
    "dfjson1.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6db2585-2384-4e35-b829-f019884a5064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. Serialized data Advanced Feature - orc, parquet/delta (very very important & we learn indepth)\n",
    "- PathOrPaths\n",
    "- **mergeSchema** - Important interview property (make it proactive/make it driven in the interview) SCHEMA EVOLUTION\n",
    "- pathGlobFilter\n",
    "- recursiveFileLookup\n",
    "- modifiedBefore\n",
    "- modifiedAfter\n",
    "Problem statement:\n",
    "Source is sending data in any way they want...\n",
    "Day1/source1- 5 cols\n",
    "Day2/source2 - 7 Cols\n",
    "\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dc511ce-2f08-4e81-b447-03a8a63ec7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Case study: If the source (external) is sending data to us, if our consumer (Datascience/Dataanalytics) is directly communicated with our source system and asked them to propogate more/less attributes/features without the knowledge of the Dataengineering team? Here we have implement the strategy of Schema Evolution using option mergeSchema\n",
    "#Story building for interview: We get product data from source which get evolved on a frequent basis for eg. product data originally sent without gendra, costprice, purchaseprice, profit/loss metrics, demant information...\n",
    "#Steps to follow:\n",
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a319628-b6c3-4f59-acda-d96c1720404f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "day1df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/day1.txt\",header=True,inferSchema=True)\n",
    "day1df.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput\",mode='append')\n",
    "day2df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/day2.txt\",header=True,inferSchema=True)\n",
    "day2df.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput\",mode='append')\n",
    "day3df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/day3.txt\",header=True,inferSchema=True)\n",
    "day3df.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6ca0a3-f039-4269-825c-c2be3cb781c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created...\n",
    "post_day3=spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcoutput/\",mergeSchema=True)\n",
    "display(post_day3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5775ca1a-ef61-45ab-9dee-78a0fdad9714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Collect the data as it is from the source\n",
    "#2. Convert into orc/parquet format and write to the target by appending the data on a regular interval\n",
    "#Here we use inferSchema without an option and We can't use structuretype, because schema is evolving...\n",
    "day1df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/day1.txt\",header=True,inferSchema=True)\n",
    "day1df.write.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput\",mode='append')\n",
    "day2df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/day2.txt\",header=True,inferSchema=True)\n",
    "day2df.write.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput\",mode='append')\n",
    "day3df=spark.read.csv(\"/Volumes/workspace/default/volumewd36/day3.txt\",header=True,inferSchema=True)\n",
    "day3df.write.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74c892d2-1d6b-4edb-a535-f35979828837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Read the data from the target and do the schema evolution and get the evolved dataframe created...\n",
    "post_day3=spark.read.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetoutput/\",mergeSchema=True)\n",
    "display(post_day3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c793a1ba-d061-45d9-852d-88a5598cd125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4. Reading data from other formats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c69af82-91aa-4854-b6ce-b1eb57c70629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Reading csv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4039ed-9dec-418d-b54b-a2fcfbd0000a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/target/csvout\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748f2729-c766-4363-ab86-b4d73205c76c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Reading json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f1957d-69c5-4d4e-b452-99bb5683cc24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.json(\"/Volumes/workspace/wd36schema/ingestion_volume/target/jsonout\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5df82fe-767f-4ab1-a316-15e9a3e57f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Reading xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35a6186-f61a-4c61-9f5f-c5f303fe6b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.xml(\"/Volumes/workspace/wd36schema/ingestion_volume/target/xmlout\",rowTag=\"cust\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26a3f08-51ab-4b48-89e8-0d077a3becd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4. Reading serialized data (orc/parquet/delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56846659-bd78-4f71-be22-e6f5ed57b5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/target/orcout\").show(2)\n",
    "spark.read.parquet(\"/Volumes/workspace/wd36schema/ingestion_volume/target/parquetout\").show(2)\n",
    "spark.read.format(\"delta\").load(\"/Volumes/workspace/wd36schema/ingestion_volume/target/deltaout\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d1d6b33-8169-4811-8acd-c94d1e463da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####5. Reading delta/hive table data - We will heavily learn this under Databricks (Spark)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2-Advanced-Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
